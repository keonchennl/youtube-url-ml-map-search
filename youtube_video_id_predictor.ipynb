{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>40949.000000</td>\n",
       "      <td>4.094900e+04</td>\n",
       "      <td>4.094900e+04</td>\n",
       "      <td>4.094900e+04</td>\n",
       "      <td>4.094900e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>19.972429</td>\n",
       "      <td>2.360785e+06</td>\n",
       "      <td>7.426670e+04</td>\n",
       "      <td>3.711401e+03</td>\n",
       "      <td>8.446804e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.568327</td>\n",
       "      <td>7.394114e+06</td>\n",
       "      <td>2.288853e+05</td>\n",
       "      <td>2.902971e+04</td>\n",
       "      <td>3.743049e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.490000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>2.423290e+05</td>\n",
       "      <td>5.424000e+03</td>\n",
       "      <td>2.020000e+02</td>\n",
       "      <td>6.140000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.000000</td>\n",
       "      <td>6.818610e+05</td>\n",
       "      <td>1.809100e+04</td>\n",
       "      <td>6.310000e+02</td>\n",
       "      <td>1.856000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>1.823157e+06</td>\n",
       "      <td>5.541700e+04</td>\n",
       "      <td>1.938000e+03</td>\n",
       "      <td>5.755000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>2.252119e+08</td>\n",
       "      <td>5.613827e+06</td>\n",
       "      <td>1.674420e+06</td>\n",
       "      <td>1.361580e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category_id         views         likes      dislikes  comment_count\n",
       "count  40949.000000  4.094900e+04  4.094900e+04  4.094900e+04   4.094900e+04\n",
       "mean      19.972429  2.360785e+06  7.426670e+04  3.711401e+03   8.446804e+03\n",
       "std        7.568327  7.394114e+06  2.288853e+05  2.902971e+04   3.743049e+04\n",
       "min        1.000000  5.490000e+02  0.000000e+00  0.000000e+00   0.000000e+00\n",
       "25%       17.000000  2.423290e+05  5.424000e+03  2.020000e+02   6.140000e+02\n",
       "50%       24.000000  6.818610e+05  1.809100e+04  6.310000e+02   1.856000e+03\n",
       "75%       25.000000  1.823157e+06  5.541700e+04  1.938000e+03   5.755000e+03\n",
       "max       43.000000  2.252119e+08  5.613827e+06  1.674420e+06   1.361580e+06"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_videos_data = pd.read_csv('USvideos.csv')\n",
    "us_videos_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32759, 16), (8190, 16))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting the data into training and test sets (80% training, 20% test)\n",
    "train_data, test_data = train_test_split(us_videos_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt from cache at C:\\Users\\Keon/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\vocab.txt\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at C:\\Users\\Keon/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\tokenizer_config.json\n",
      "loading configuration file config.json from cache at C:\\Users\\Keon/.cache\\huggingface\\hub\\models--bert-base-uncased\\snapshots\\1dbc166cf8765166998eff31ade2eb64c8a40076\\config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.22.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize titles\n",
    "train_encodings = tokenizer(train_data['title'].tolist(), truncation=True, padding=True, max_length=50)\n",
    "test_encodings = tokenizer(test_data['title'].tolist(), truncation=True, padding=True, max_length=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32650</th>\n",
       "      <td>PAHm7SEhhuw</td>\n",
       "      <td>18.04.05</td>\n",
       "      <td>Boxing with Evander Holyfield &amp; Joel McHale | ...</td>\n",
       "      <td>LOL Network</td>\n",
       "      <td>23</td>\n",
       "      <td>2018-04-26T17:00:02.000Z</td>\n",
       "      <td>What the Fit|\"Kevin Hart What the Fit\"|\"Kevin ...</td>\n",
       "      <td>1103235</td>\n",
       "      <td>20418</td>\n",
       "      <td>1125</td>\n",
       "      <td>1238</td>\n",
       "      <td>https://i.ytimg.com/vi/PAHm7SEhhuw/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>If there’s one thing that boxing legend Evande...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29588</th>\n",
       "      <td>-xAMnJgc3oA</td>\n",
       "      <td>18.19.04</td>\n",
       "      <td>Janelle Monáe – I Like That [Official Audio]</td>\n",
       "      <td>Janelle Monáe</td>\n",
       "      <td>10</td>\n",
       "      <td>2018-04-16T04:38:11.000Z</td>\n",
       "      <td>Janelle Monae|\"Wonderland\"|\"Janelle\"|\"Electric...</td>\n",
       "      <td>382903</td>\n",
       "      <td>16787</td>\n",
       "      <td>245</td>\n",
       "      <td>1060</td>\n",
       "      <td>https://i.ytimg.com/vi/-xAMnJgc3oA/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>“I Like That”, “Make Me Feel”, “Django Jane” &amp;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20521</th>\n",
       "      <td>OKXLBEvGgTA</td>\n",
       "      <td>18.26.02</td>\n",
       "      <td>I Tried Following a Safiya Nygaard Makeup Tuto...</td>\n",
       "      <td>Sylvia Gani</td>\n",
       "      <td>26</td>\n",
       "      <td>2018-02-19T01:30:53.000Z</td>\n",
       "      <td>i tried following a makeup tutorial|\"I tried f...</td>\n",
       "      <td>799025</td>\n",
       "      <td>32450</td>\n",
       "      <td>763</td>\n",
       "      <td>2393</td>\n",
       "      <td>https://i.ytimg.com/vi/OKXLBEvGgTA/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SUBSCRIBE HERE: http://bit.ly/2g1cqFW\\nWatch t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>Wen6VQS6NG4</td>\n",
       "      <td>17.24.11</td>\n",
       "      <td>Giant Mousetrap powered Car</td>\n",
       "      <td>TheBackyardScientist</td>\n",
       "      <td>28</td>\n",
       "      <td>2017-11-23T02:47:34.000Z</td>\n",
       "      <td>mousetrap car|\"thebackyardscientist\"|\"Mousetra...</td>\n",
       "      <td>588908</td>\n",
       "      <td>16422</td>\n",
       "      <td>1238</td>\n",
       "      <td>1636</td>\n",
       "      <td>https://i.ytimg.com/vi/Wen6VQS6NG4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>This is just a mini-video to announce i've bee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12586</th>\n",
       "      <td>gsVgqms1slE</td>\n",
       "      <td>18.17.01</td>\n",
       "      <td>Rag'n'Bone Man - Die Easy (Official Video)</td>\n",
       "      <td>RagnBoneManVEVO</td>\n",
       "      <td>10</td>\n",
       "      <td>2018-01-11T08:00:01.000Z</td>\n",
       "      <td>Alternative/Indie|\"Columbia\"|\"Die Easy\"|\"Rag'n...</td>\n",
       "      <td>299897</td>\n",
       "      <td>13180</td>\n",
       "      <td>294</td>\n",
       "      <td>626</td>\n",
       "      <td>https://i.ytimg.com/vi/gsVgqms1slE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Music video by Rag'n'Bone Man performing Die E...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6265</th>\n",
       "      <td>3j5eGupDWY4</td>\n",
       "      <td>17.15.12</td>\n",
       "      <td>Homeless and abused, this Pit Bull didn't lose...</td>\n",
       "      <td>Hope For Paws - Official Rescue Channel</td>\n",
       "      <td>15</td>\n",
       "      <td>2017-12-12T17:01:47.000Z</td>\n",
       "      <td>Loreta Frankonyte|\"Eldad Hagar\"|\"Hope For Paws\"</td>\n",
       "      <td>394927</td>\n",
       "      <td>22272</td>\n",
       "      <td>167</td>\n",
       "      <td>1366</td>\n",
       "      <td>https://i.ytimg.com/vi/3j5eGupDWY4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>We need your support this holiday season!  Ple...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11284</th>\n",
       "      <td>aCoDqL1dK9c</td>\n",
       "      <td>18.09.01</td>\n",
       "      <td>BEST MAKEUP OF 2017!</td>\n",
       "      <td>NikkieTutorials</td>\n",
       "      <td>26</td>\n",
       "      <td>2018-01-05T21:00:29.000Z</td>\n",
       "      <td>best makeup of 2017|\"best makeup\"|\"best makeup...</td>\n",
       "      <td>1528291</td>\n",
       "      <td>159801</td>\n",
       "      <td>723</td>\n",
       "      <td>78990</td>\n",
       "      <td>https://i.ytimg.com/vi/aCoDqL1dK9c/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Make sure you subscribe to my channel and hit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38158</th>\n",
       "      <td>gS1DbvHHVH0</td>\n",
       "      <td>18.01.06</td>\n",
       "      <td>Going in to brain surgery</td>\n",
       "      <td>Simone Giertz</td>\n",
       "      <td>28</td>\n",
       "      <td>2018-05-30T14:22:13.000Z</td>\n",
       "      <td>[none]</td>\n",
       "      <td>1250826</td>\n",
       "      <td>109170</td>\n",
       "      <td>825</td>\n",
       "      <td>19039</td>\n",
       "      <td>https://i.ytimg.com/vi/gS1DbvHHVH0/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>We’ll post an update on Instagram and Twitter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>svZScJMh4Pg</td>\n",
       "      <td>17.18.11</td>\n",
       "      <td>Dua Lipa - Golden Slumbers</td>\n",
       "      <td>Dua Lipa</td>\n",
       "      <td>10</td>\n",
       "      <td>2017-11-16T17:02:13.000Z</td>\n",
       "      <td>dua lipa|\"golden slumbers\"|\"xmas songs\"|\"chris...</td>\n",
       "      <td>441164</td>\n",
       "      <td>36355</td>\n",
       "      <td>560</td>\n",
       "      <td>2482</td>\n",
       "      <td>https://i.ytimg.com/vi/svZScJMh4Pg/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Production By Dan Heath and Superhuman\\n\\n\\nMY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15795</th>\n",
       "      <td>sefscV3GvWM</td>\n",
       "      <td>18.03.02</td>\n",
       "      <td>Good Odds | Toyota</td>\n",
       "      <td>Toyota Global</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-02-02T13:00:22.000Z</td>\n",
       "      <td>Toyota|\"Olympics\"|\"Paralympics\"|\"PyeongChang 2...</td>\n",
       "      <td>64002</td>\n",
       "      <td>1928</td>\n",
       "      <td>125</td>\n",
       "      <td>183</td>\n",
       "      <td>https://i.ytimg.com/vi/sefscV3GvWM/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>The odds of winning a Paralympic gold medal ar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32759 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          video_id trending_date  \\\n",
       "32650  PAHm7SEhhuw      18.04.05   \n",
       "29588  -xAMnJgc3oA      18.19.04   \n",
       "20521  OKXLBEvGgTA      18.26.02   \n",
       "2008   Wen6VQS6NG4      17.24.11   \n",
       "12586  gsVgqms1slE      18.17.01   \n",
       "...            ...           ...   \n",
       "6265   3j5eGupDWY4      17.15.12   \n",
       "11284  aCoDqL1dK9c      18.09.01   \n",
       "38158  gS1DbvHHVH0      18.01.06   \n",
       "860    svZScJMh4Pg      17.18.11   \n",
       "15795  sefscV3GvWM      18.03.02   \n",
       "\n",
       "                                                   title  \\\n",
       "32650  Boxing with Evander Holyfield & Joel McHale | ...   \n",
       "29588       Janelle Monáe – I Like That [Official Audio]   \n",
       "20521  I Tried Following a Safiya Nygaard Makeup Tuto...   \n",
       "2008                         Giant Mousetrap powered Car   \n",
       "12586         Rag'n'Bone Man - Die Easy (Official Video)   \n",
       "...                                                  ...   \n",
       "6265   Homeless and abused, this Pit Bull didn't lose...   \n",
       "11284                               BEST MAKEUP OF 2017!   \n",
       "38158                          Going in to brain surgery   \n",
       "860                           Dua Lipa - Golden Slumbers   \n",
       "15795                                 Good Odds | Toyota   \n",
       "\n",
       "                                 channel_title  category_id  \\\n",
       "32650                              LOL Network           23   \n",
       "29588                            Janelle Monáe           10   \n",
       "20521                              Sylvia Gani           26   \n",
       "2008                      TheBackyardScientist           28   \n",
       "12586                          RagnBoneManVEVO           10   \n",
       "...                                        ...          ...   \n",
       "6265   Hope For Paws - Official Rescue Channel           15   \n",
       "11284                          NikkieTutorials           26   \n",
       "38158                            Simone Giertz           28   \n",
       "860                                   Dua Lipa           10   \n",
       "15795                            Toyota Global            2   \n",
       "\n",
       "                   publish_time  \\\n",
       "32650  2018-04-26T17:00:02.000Z   \n",
       "29588  2018-04-16T04:38:11.000Z   \n",
       "20521  2018-02-19T01:30:53.000Z   \n",
       "2008   2017-11-23T02:47:34.000Z   \n",
       "12586  2018-01-11T08:00:01.000Z   \n",
       "...                         ...   \n",
       "6265   2017-12-12T17:01:47.000Z   \n",
       "11284  2018-01-05T21:00:29.000Z   \n",
       "38158  2018-05-30T14:22:13.000Z   \n",
       "860    2017-11-16T17:02:13.000Z   \n",
       "15795  2018-02-02T13:00:22.000Z   \n",
       "\n",
       "                                                    tags    views   likes  \\\n",
       "32650  What the Fit|\"Kevin Hart What the Fit\"|\"Kevin ...  1103235   20418   \n",
       "29588  Janelle Monae|\"Wonderland\"|\"Janelle\"|\"Electric...   382903   16787   \n",
       "20521  i tried following a makeup tutorial|\"I tried f...   799025   32450   \n",
       "2008   mousetrap car|\"thebackyardscientist\"|\"Mousetra...   588908   16422   \n",
       "12586  Alternative/Indie|\"Columbia\"|\"Die Easy\"|\"Rag'n...   299897   13180   \n",
       "...                                                  ...      ...     ...   \n",
       "6265     Loreta Frankonyte|\"Eldad Hagar\"|\"Hope For Paws\"   394927   22272   \n",
       "11284  best makeup of 2017|\"best makeup\"|\"best makeup...  1528291  159801   \n",
       "38158                                             [none]  1250826  109170   \n",
       "860    dua lipa|\"golden slumbers\"|\"xmas songs\"|\"chris...   441164   36355   \n",
       "15795  Toyota|\"Olympics\"|\"Paralympics\"|\"PyeongChang 2...    64002    1928   \n",
       "\n",
       "       dislikes  comment_count  \\\n",
       "32650      1125           1238   \n",
       "29588       245           1060   \n",
       "20521       763           2393   \n",
       "2008       1238           1636   \n",
       "12586       294            626   \n",
       "...         ...            ...   \n",
       "6265        167           1366   \n",
       "11284       723          78990   \n",
       "38158       825          19039   \n",
       "860         560           2482   \n",
       "15795       125            183   \n",
       "\n",
       "                                       thumbnail_link  comments_disabled  \\\n",
       "32650  https://i.ytimg.com/vi/PAHm7SEhhuw/default.jpg              False   \n",
       "29588  https://i.ytimg.com/vi/-xAMnJgc3oA/default.jpg              False   \n",
       "20521  https://i.ytimg.com/vi/OKXLBEvGgTA/default.jpg              False   \n",
       "2008   https://i.ytimg.com/vi/Wen6VQS6NG4/default.jpg              False   \n",
       "12586  https://i.ytimg.com/vi/gsVgqms1slE/default.jpg              False   \n",
       "...                                               ...                ...   \n",
       "6265   https://i.ytimg.com/vi/3j5eGupDWY4/default.jpg              False   \n",
       "11284  https://i.ytimg.com/vi/aCoDqL1dK9c/default.jpg              False   \n",
       "38158  https://i.ytimg.com/vi/gS1DbvHHVH0/default.jpg              False   \n",
       "860    https://i.ytimg.com/vi/svZScJMh4Pg/default.jpg              False   \n",
       "15795  https://i.ytimg.com/vi/sefscV3GvWM/default.jpg              False   \n",
       "\n",
       "       ratings_disabled  video_error_or_removed  \\\n",
       "32650             False                   False   \n",
       "29588             False                   False   \n",
       "20521             False                   False   \n",
       "2008              False                   False   \n",
       "12586             False                   False   \n",
       "...                 ...                     ...   \n",
       "6265              False                   False   \n",
       "11284             False                   False   \n",
       "38158             False                   False   \n",
       "860               False                   False   \n",
       "15795             False                   False   \n",
       "\n",
       "                                             description  \n",
       "32650  If there’s one thing that boxing legend Evande...  \n",
       "29588  “I Like That”, “Make Me Feel”, “Django Jane” &...  \n",
       "20521  SUBSCRIBE HERE: http://bit.ly/2g1cqFW\\nWatch t...  \n",
       "2008   This is just a mini-video to announce i've bee...  \n",
       "12586  Music video by Rag'n'Bone Man performing Die E...  \n",
       "...                                                  ...  \n",
       "6265   We need your support this holiday season!  Ple...  \n",
       "11284  Make sure you subscribe to my channel and hit ...  \n",
       "38158  We’ll post an update on Instagram and Twitter ...  \n",
       "860    Production By Dan Heath and Superhuman\\n\\n\\nMY...  \n",
       "15795  The odds of winning a Paralympic gold medal ar...  \n",
       "\n",
       "[32759 rows x 16 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32759, 6171), (8190, 6171))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One-hot coding for the video ids\n",
    "# One-hot encoding doesn't work since the model expects 1 integer instead of a vector(one-hot encoding) as label\n",
    "lb = LabelBinarizer()\n",
    "train_labels = lb.fit_transform(train_data['video_id'])\n",
    "test_labels = lb.transform(test_data['video_id'])\n",
    "\n",
    "train_labels.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6171"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lb.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert video_id to class indices\n",
    "train_label_indices = train_data['video_id'].map(lambda x: list(lb.classes_).index(x)).values\n",
    "# test_label_indices = test_data['video_id'].map(lambda x: list(lb.classes_).index(x)).values # doesn't work because every label is unique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YouTubeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = YouTubeDataset(train_encodings, train_label_indices)\n",
    "# val_dataset = YouTubeDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([  101,  8362,  2007,  9340,  4063,  4151,  3790,  1004,  8963, 11338,\n",
       "          15238,  1064,  4901,  7530,  1024,  2054,  1996,  4906,  4958,  1022,\n",
       "           1064,  4756,  2041,  5189,  2897,   102,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]),\n",
       "  'labels': tensor(2549)},\n",
       " {'input_ids': tensor([  101,  4869,  6216, 13813,  2063,  1516,  1045,  2066,  2008,  1031,\n",
       "           2880,  5746,  1033,   102,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       "  'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]),\n",
       "  'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0]),\n",
       "  'labels': tensor(86)})"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0], train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBertForSequenceClassification\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(lb\u001b[38;5;241m.\u001b[39mclasses_))\n\u001b[0;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      4\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m      5\u001b[0m     per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     12\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     13\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     14\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[0;32m     15\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset\n\u001b[0;32m     16\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(lb.classes_))\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     num_train_epochs=5,\n",
    "#     logging_dir='./logs',\n",
    "#     output_dir='./output'\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_dataset,\n",
    "#     eval_dataset=val_dataset\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m new_titles \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWE WANT TO TALK ABOUT OUR MARRIAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpotmini\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m new_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(new_titles, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mYouTubeDataset\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, encodings, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "new_titles = [\"WE WANT TO TALK ABOUT OUR MARRIAGE\", \"Spotmini\"]\n",
    "new_encodings = tokenizer(new_titles, truncation=True, padding=True, max_length=50)\n",
    "\n",
    "class YouTubeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "\n",
    "new_dataset = YouTubeDataset(new_encodings, labels=None)\n",
    "predictions = trainer.predict(new_dataset)\n",
    "predicted_class_indices = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "# Convert class indices back to video IDs\n",
    "predicted_video_ids = [lb.classes_[index] for index in predicted_class_indices]\n",
    "# predictions\n",
    "print(predicted_video_ids)\n",
    "\n",
    "# Get the predicted titles based on the video IDs\n",
    "predicted_titles = train_data[train_data['video_id'].isin(predicted_video_ids)]['title'].tolist()\n",
    "print(predicted_titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Youtube video titled \"WE WANT TO TALK ABOUT OUR MARRIAGE\" can be found at https://www.youtube.com/watch?v=2kyS6SvSYSE\n",
    "- the Youtube video titled \"The Trump Presidency: Last Week Tonight with John Oliver (HBO)\" can be found at https://www.youtube.com/watch?v=1ZAPwfrtAFY\n",
    "- the Youtube video titled \"Racist Superman | Rudy Mancuso, King Bach & Lele Pons\" can be found at https://www.youtube.com/watch?v=5qpjK5DgCt4\n",
    "- the Youtube video titles \"Nickelback Lyrics: Real or Fake?\" can be found at https://www.youtube.com/watch?v=puqaWrEC7tY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.17.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>4737269</td>\n",
       "      <td>175762</td>\n",
       "      <td>7017</td>\n",
       "      <td>9557</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.18.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>4949674</td>\n",
       "      <td>180201</td>\n",
       "      <td>7042</td>\n",
       "      <td>9687</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.15.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>4326684</td>\n",
       "      <td>167696</td>\n",
       "      <td>6730</td>\n",
       "      <td>9265</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        video_id trending_date  \\\n",
       "713  5qpjK5DgCt4      17.17.11   \n",
       "2    5qpjK5DgCt4      17.14.11   \n",
       "954  5qpjK5DgCt4      17.18.11   \n",
       "238  5qpjK5DgCt4      17.15.11   \n",
       "\n",
       "                                                 title channel_title  \\\n",
       "713  Racist Superman | Rudy Mancuso, King Bach & Le...  Rudy Mancuso   \n",
       "2    Racist Superman | Rudy Mancuso, King Bach & Le...  Rudy Mancuso   \n",
       "954  Racist Superman | Rudy Mancuso, King Bach & Le...  Rudy Mancuso   \n",
       "238  Racist Superman | Rudy Mancuso, King Bach & Le...  Rudy Mancuso   \n",
       "\n",
       "     category_id              publish_time  \\\n",
       "713           23  2017-11-12T19:05:24.000Z   \n",
       "2             23  2017-11-12T19:05:24.000Z   \n",
       "954           23  2017-11-12T19:05:24.000Z   \n",
       "238           23  2017-11-12T19:05:24.000Z   \n",
       "\n",
       "                                                  tags    views   likes  \\\n",
       "713  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  4737269  175762   \n",
       "2    racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n",
       "954  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  4949674  180201   \n",
       "238  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  4326684  167696   \n",
       "\n",
       "     dislikes  comment_count                                  thumbnail_link  \\\n",
       "713      7017           9557  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "2        5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "954      7042           9687  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "238      6730           9265  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "\n",
       "     comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "713              False             False                   False   \n",
       "2                False             False                   False   \n",
       "954              False             False                   False   \n",
       "238              False             False                   False   \n",
       "\n",
       "                                           description  \n",
       "713  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  \n",
       "2    WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  \n",
       "954  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  \n",
       "238  WATCH MY PREVIOUS VIDEO ▶ \\n\\nSUBSCRIBE ► http...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[train_data['video_id']=='5qpjK5DgCt4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2kyS6SvSYSE', '1ZAPwfrtAFY', 'nsZvmFUXkrI', 'puqaWrEC7tY']\n",
      "['WE WANT TO TALK ABOUT OUR MARRIAGE', 'The Trump Presidency: Last Week Tonight with John Oliver (HBO)', 'Racist Superman | Rudy Mancuso, Alesso & King Bach', 'Nickelback Lyrics: Real or Fake?']\n"
     ]
    }
   ],
   "source": [
    "# For prediction\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_path = \"output/checkpoint-9500\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "new_titles = [\"WE WANT TO TALK ABOUT OUR MARRIAGE\", \"The Trump Presidency: Last Week Tonight with John Oliver (HBO)\", \"Racist Superman | Rudy Mancuso, King Bach & Lele Pons\", \"Nickelback Lyrics: Real or Fake?\"]\n",
    "new_encodings = tokenizer(new_titles, truncation=True, padding=True, max_length=50)\n",
    "\n",
    "class YouTubeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "\n",
    "new_dataset = YouTubeDataset(new_encodings, labels=None)\n",
    "trainer = Trainer(model=model)\n",
    "predictions = trainer.predict(new_dataset)\n",
    "predicted_class_indices = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "# Convert class indices back to video IDs\n",
    "predicted_video_ids = [lb.classes_[index] for index in predicted_class_indices]\n",
    "# predictions\n",
    "print(predicted_video_ids)\n",
    "\n",
    "# Create a dictionary to map video_id to title\n",
    "video_id_to_title = dict(zip(us_videos_data['video_id'], us_videos_data['title']))\n",
    "\n",
    "# Get the predicted titles using the dictionary\n",
    "predicted_titles = [video_id_to_title[video_id] for video_id in predicted_video_ids if video_id in video_id_to_title]\n",
    "print(predicted_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# 1. Replace new_titles with all titles from train_data\n",
    "new_titles = train_data['title'].tolist()\n",
    "new_encodings = tokenizer(new_titles, truncation=True, padding=True, max_length=50)\n",
    "\n",
    "# 2. Convert tokenized data to a PyTorch dataset and predict\n",
    "new_dataset = YouTubeDataset(new_encodings, labels=None)\n",
    "predictions = trainer.predict(new_dataset)\n",
    "predicted_class_indices = predictions.predictions.argmax(axis=1)\n",
    "\n",
    "# Convert class indices back to video IDs\n",
    "predicted_video_ids = [lb.classes_[index] for index in predicted_class_indices]\n",
    "\n",
    "# 3. Compare these predicted IDs with the actual IDs from train_data to calculate precision\n",
    "true_video_ids = train_data['video_id'].tolist()\n",
    "precision_val = precision_score(true_video_ids, predicted_video_ids, average='micro')\n",
    "\n",
    "# Results\n",
    "# predicted_video_ids\n",
    "precision_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try Using GPT2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')  # You can choose the model size: 'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'\n",
    "model_path = \"C:/Users/Keon/Downloads/0_2023_Thesis/LLM/nanoGPT/gpt2-medium\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "\n",
    "# Use titles from train_data as input\n",
    "new_titles = train_data['title'].tolist()\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "new_encodings = tokenizer(new_titles, return_tensors='pt', truncation=True, padding=True, max_length=50).input_ids\n",
    "\n",
    "# GPT-2 doesn't use the traditional label-input format like BERT. It self-regresses to predict the next token in the sequence.\n",
    "labels = new_encodings.clone()\n",
    "\n",
    "# Move model to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "new_encodings = new_encodings.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# Generate predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(new_encodings, max_length=50, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)\n",
    "    predicted_video_ids = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "\n",
    "# Compare predicted video IDs with actual video IDs to calculate precision\n",
    "true_video_ids = train_data['video_id'].tolist()\n",
    "precision_val = precision_score(true_video_ids, predicted_video_ids, average='micro')\n",
    "\n",
    "# Print results\n",
    "print(\"Predicted Video IDs:\", predicted_video_ids)\n",
    "print(\"Precision:\", precision_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
